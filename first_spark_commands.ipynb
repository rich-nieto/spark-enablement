{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afee604c-bba2-4ed7-82f5-46b535e7d644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg,col,countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91c56487-342f-46b9-b307-5efae2930494",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/28 16:28:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Spark session\n",
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .appName('AuthorsAges')\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f18f9d2e-b2d2-4742-973d-1f27252c6322",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "data_df = spark.createDataFrame([(\"Brooke\", 20),\n",
    "                                 (\"Denny\", 31),\n",
    "                                 (\"Jules\", 30),\n",
    "                                 (\"TD\", 35),\n",
    "                                 (\"Brooke\", 25)], [\"name\",\"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33f88d0f-47c0-4cdb-9d52-953ca940797c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|avg(age)|\n",
      "+------+--------+\n",
      "|Brooke|    22.5|\n",
      "| Denny|    31.0|\n",
      "| Jules|    30.0|\n",
      "|    TD|    35.0|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group same names together, aggregate their ages, and computer average\n",
    "avg_df = data_df.groupBy(\"name\").agg(avg(\"age\"))\n",
    "avg_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17481fd1-1c1f-4a2d-95af-23e5778b74b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Brooke| 20|\n",
      "| Denny| 31|\n",
      "| Jules| 30|\n",
      "|    TD| 35|\n",
      "|Brooke| 25|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42b8672-329f-4bd0-8241-f533980eaf1c",
   "metadata": {},
   "source": [
    "## Creating DataFrames\n",
    "\n",
    "- A *schema* defines the column names and associated data types for a Spark DataFrame\n",
    "- You can let Spark infer the dtypes, but it's better to explicitly do it ahead of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f8f5747-a900-4666-a9a2-2d869c176c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create our data\n",
    "data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\", \"LinkedIn\"]],\n",
    "       [2, \"Brooke\",\"Wenig\",\"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\", \"LinkedIn\"]],\n",
    "       [3, \"Denny\", \"Lee\", \"https://tinyurl.3\",\"6/7/2019\",7659, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "       [4, \"Tathagata\", \"Das\",\"https://tinyurl.4\", \"5/12/2018\", 10568, [\"twitter\", \"FB\"]],\n",
    "       [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "       [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, [\"twitter\", \"LinkedIn\"]]\n",
    "      ]\n",
    "# specify schema\n",
    "schema = \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING, `Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "839e10c3-e0d0-4e17-a355-2b15c6b4ef40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs_df = (spark\n",
    "           .createDataFrame(data, schema))\n",
    "blogs_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90f6c4b1-fc8d-4c76-b785-e69847084b8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- Hits: integer (nullable = true)\n",
      " |-- Campaigns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# show schema\n",
    "print(blogs_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd0dd750-2481-44c7-85c7-ec01ce888212",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('Id', IntegerType(), True), StructField('First', StringType(), True), StructField('Last', StringType(), True), StructField('Url', StringType(), True), StructField('Published', StringType(), True), StructField('Hits', IntegerType(), True), StructField('Campaigns', ArrayType(StringType(), True), True)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can reuse this schema else in the code\n",
    "blogs_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655b589a-70bb-43f8-ba63-69f4f6c2e36c",
   "metadata": {},
   "source": [
    "## Reading from file\n",
    "\n",
    "- Could just let Spark infer, but will specify schema\n",
    "- Complex so will let Spark infer from a tiny sample first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e942a4ce-506c-46ad-8252-d7b05a8eb4a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sf_fire_file = \"../LearningSparkV2/databricks-datasets/learning-spark-v2/sf-fire/sf-fire-calls.csv\"\n",
    "# get sample df for schema extraction\n",
    "sample_df = (spark\n",
    "            .read\n",
    "            .option(\"samplingRatio\", 0.001)\n",
    "            .option(\"header\", True)\n",
    "            .csv(sf_fire_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9a47367-2544-432a-a6a8-478dc54be5b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read in full data and specify schema\n",
    "fire_df = (spark\n",
    "          .read\n",
    "          .csv(sf_fire_file, header=True, \n",
    "               schema=sample_df.schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd7654d6-e641-47c3-9c82-fcaab6baf3d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/28 17:44:26 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/04/28 17:44:26 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/04/28 17:44:26 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "23/04/28 17:44:26 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "23/04/28 17:44:26 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "23/04/28 17:44:27 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/04/28 17:44:27 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# write data as parquet file\n",
    "parquet_path ='./sample_parquet'\n",
    "fire_df.write.format(\"parquet\").save(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a3a9074-ca15-4c90-826c-08b7aba4a778",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+--------------+\n",
      "|IncidentNumber|AvailableDtTm         |CallType      |\n",
      "+--------------+----------------------+--------------+\n",
      "|2003235       |01/11/2002 01:51:44 AM|Structure Fire|\n",
      "|2003250       |01/11/2002 04:16:46 AM|Vehicle Fire  |\n",
      "|2003259       |01/11/2002 06:01:58 AM|Alarms        |\n",
      "|2003279       |01/11/2002 08:03:26 AM|Structure Fire|\n",
      "|2003301       |01/11/2002 09:46:44 AM|Alarms        |\n",
      "+--------------+----------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# projects and filters\n",
    "few_fire_df = (fire_df\n",
    "              .select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\")\n",
    "              .where(col(\"CallType\") != \"Medical Incident\"))\n",
    "few_fire_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6bc87812-e120-47be-af53-c30cce9aacd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:=====>                                                   (1 + 9) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DistinctCallTypes|\n",
      "+-----------------+\n",
      "|               30|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# projects and filters\n",
    "_ = (fire_df\n",
    " .select(\"CallType\")\n",
    " .where(col(\"CallType\").isNotNull())\n",
    " .agg(countDistinct(\"CallType\").alias(\"DistinctCallTypes\")))\n",
    "_.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ae0d52f-a116-4f85-b6b8-cce8f36e2a16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|CallType                     |\n",
      "+-----------------------------+\n",
      "|Elevator / Escalator Rescue  |\n",
      "|Marine Fire                  |\n",
      "|Aircraft Emergency           |\n",
      "|Administrative               |\n",
      "|Alarms                       |\n",
      "|Odor (Strange / Unknown)     |\n",
      "|Citizen Assist / Service Call|\n",
      "|HazMat                       |\n",
      "|Explosion                    |\n",
      "|Oil Spill                    |\n",
      "+-----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(fire_df\n",
    ".select(\"CallType\")\n",
    ".where(col(\"CallType\").isNotNull())\n",
    ".distinct()\n",
    ".show(10, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e42d92-cc96-44e7-9756-27a0835f8686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
